#!/usr/bin/env python3
# scripts/system_tests/coverage_report.py
# ============================================================================
# Module: Coverage Report Generator
# Description: Generate Decision Gate system-test coverage docs.
# Purpose: Turn registry + gaps into published documentation.
# Dependencies: stdlib, tomllib (or toml)
# ============================================================================

from __future__ import annotations

import argparse
import sys
from datetime import datetime
from datetime import timezone
from pathlib import Path
from typing import Any, Dict, List, MutableMapping, cast

try:
    import tomllib as toml  # Python 3.11+
except ModuleNotFoundError:  # pragma: no cover
    try:
        import toml  # type: ignore
    except ModuleNotFoundError:
        print("Error: tomllib not available. Install 'toml' with: pip install toml")
        sys.exit(1)

RegistryData = MutableMapping[str, Any]
TestEntry = Dict[str, Any]
GapEntry = Dict[str, Any]
CategoryEntry = Dict[str, Any]


def load_toml(path: Path) -> RegistryData:
    """Read and parse a TOML file."""
    if not path.exists():
        raise FileNotFoundError(f"TOML file not found: {path}")
    return toml.loads(path.read_text(encoding="utf-8"))


def render_table(headers: List[str], rows: List[List[str]]) -> str:
    """Render a Markdown table from headers and rows."""
    line = "| " + " | ".join(headers) + " |"
    sep = "|" + "|".join([" --- " for _ in headers]) + "|"
    out = [line, sep]
    for row in rows:
        out.append("| " + " | ".join(row) + " |")
    return "\n".join(out)


def generate_coverage_report(registry: RegistryData, gaps: RegistryData) -> str:
    """Build the Markdown coverage report from registry and gaps data."""
    tests: List[TestEntry] = list(registry.get("tests", []))
    categories: Dict[str, CategoryEntry] = cast(
        Dict[str, CategoryEntry], registry.get("categories", {})
    )
    gap_entries: List[GapEntry] = list(gaps.get("gaps", []))

    by_category: Dict[str, int] = {}
    for test in tests:
        by_category[test.get("category", "unknown")] = (
            by_category.get(test.get("category", "unknown"), 0) + 1
        )

    p0_tests = [test for test in tests if test.get("priority") == "P0"]
    p1_tests = [test for test in tests if test.get("priority") == "P1"]
    p2_tests = [test for test in tests if test.get("priority") == "P2"]

    gap_open = [gap for gap in gap_entries if gap.get("status") != "closed"]

    rows: List[List[str]] = []
    for test in p0_tests:
        rows.append(
            [
                test["name"],
                test.get("category", "unknown"),
                str(test.get("estimated_runtime_sec", 0)),
                f"`{test.get('run_command', '')}`",
            ]
        )

    gaps_by_category: Dict[str, int] = {}
    for gap in gap_open:
        gaps_by_category[gap.get("category", "unknown")] = (
            gaps_by_category.get(gap.get("category", "unknown"), 0) + 1
        )

    all_categories = set(by_category) | set(gaps_by_category)
    category_rows: List[List[str]] = []
    for cat in sorted(all_categories):
        closed = by_category.get(cat, 0)
        total = closed + gaps_by_category.get(cat, 0)
        percent = int((closed / total) * 100) if total else 0
        coverage = f"{closed}/{total} ({percent}%)"
        category_rows.append([cat, coverage, categories.get(cat, {}).get("description", "-")])

    gap_rows: List[List[str]] = []
    for gap in gap_open:
        gap_rows.append(
            [
                gap.get("id", ""),
                gap.get("title", ""),
                gap.get("priority", ""),
                gap.get("category", ""),
                gap.get("status", ""),
            ]
        )

    total_closed = len(tests)
    total_open = len(gap_open)
    total_all = total_closed + total_open
    total_percent = int((total_closed / total_all) * 100) if total_all else 0

    gap_p0 = [gap for gap in gap_open if gap.get("priority") == "P0"]
    gap_p1 = [gap for gap in gap_open if gap.get("priority") == "P1"]
    gap_p2 = [gap for gap in gap_open if gap.get("priority") == "P2"]

    p0_total = len(p0_tests) + len(gap_p0)
    p1_total = len(p1_tests) + len(gap_p1)
    p2_total = len(p2_tests) + len(gap_p2)

    p0_percent = int((len(p0_tests) / p0_total) * 100) if p0_total else 0
    p1_percent = int((len(p1_tests) / p1_total) * 100) if p1_total else 0
    p2_percent = int((len(p2_tests) / p2_total) * 100) if p2_total else 0

    report = f"""# Decision Gate System-Test Coverage (Auto-Generated)

**Generated:** {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')} UTC
**Source:** system-tests/test_registry.toml + system-tests/test_gaps.toml

> This document is auto-generated by `scripts/system_tests/coverage_report.py`.

## Executive Summary

- **Total Tests:** {len(tests)}
- **P0:** {len(p0_tests)}
- **P1:** {len(p1_tests)}
- **P2:** {len(p2_tests)}
- **Categories:** {len(by_category)}
- **Open Gaps:** {len(gap_open)}
- **Coverage (Closed/Total):** {total_closed}/{total_all} ({total_percent}%)
- **P0 Coverage:** {len(p0_tests)}/{p0_total} ({p0_percent}%)
- **P1 Coverage:** {len(p1_tests)}/{p1_total} ({p1_percent}%)
- **P2 Coverage:** {len(p2_tests)}/{p2_total} ({p2_percent}%)

## P0 Tests

{render_table(["Test", "Category", "Est. Runtime (s)", "Run Command"], rows)}

## Category Coverage

    {render_table(["Category", "Coverage", "Description"], category_rows)}

"""

    if gap_rows:
        report += "\n## Open Gaps\n\n"
        report += render_table(["ID", "Title", "Priority", "Category", "Status"], gap_rows)
        report += "\n"

    return report


def generate_infrastructure_guide(registry: RegistryData) -> str:
    """Build the Markdown infrastructure guide from registry data."""
    categories: Dict[str, CategoryEntry] = cast(
        Dict[str, CategoryEntry], registry.get("categories", {})
    )
    category_rows: List[List[str]] = [
        [cat, info.get("description", "-"), "yes" if info.get("quick") else "no"]
        for cat, info in categories.items()
    ]

    guide = f"""# Decision Gate Test Infrastructure Guide (Auto-Generated)

**Generated:** {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')} UTC

> This document is auto-generated by `scripts/system_tests/coverage_report.py`.

## Overview

System-tests are registry-driven. Every test is declared in
`system-tests/test_registry.toml` and all known gaps live in
`system-tests/test_gaps.toml`.

## Running Tests

```bash
python3 scripts/system_tests/test_runner.py --priority P0
python3 scripts/system_tests/test_runner.py --category runpack
python3 scripts/system_tests/test_runner.py --category performance
python3 scripts/system_tests/test_runner.py --category performance_sqlite
```

## Environment Variables

- `DECISION_GATE_SYSTEM_TEST_RUN_ROOT`: per-test artifact root (set by runner)
- `DECISION_GATE_SYSTEM_TEST_HTTP_BIND`: optional bind override
- `DECISION_GATE_SYSTEM_TEST_PROVIDER_URL`: optional external provider URL
- `DECISION_GATE_SYSTEM_TEST_TIMEOUT_SEC`: optional timeout override

## Category Index

{render_table(["Category", "Description", "Quick"], category_rows)}

## Artifact Contract

Each system-test emits at least:
- `summary.json`
- `summary.md`
- `tool_transcript.json`

Runpack tests also emit `runpack/` with exported artifacts.

Performance tests also emit:
- `perf_summary.json` (throughput/latency/error metrics + SLO evaluation, including measured-window elapsed fields)
- `perf_tool_metrics.json` (per-tool p95 and total-time rankings)
- `perf_target.json` (resolved threshold/workload contract for the run)
- `sqlite_config.json` / `sqlite_sweep.json` for SQLite-track configuration + sweep output
- `sqlite_contention.json` for SQLite store microbench contention counters
- `writer_diagnostics.json` for SQLite writer queue and batch diagnostics
- `mutation_diagnostics.json` for MCP per-run mutation coordinator diagnostics

## Performance Operating Model

- Absolute SLO thresholds are defined in `system-tests/perf_targets.toml`.
- SQLite track thresholds and workload params are defined in
  `system-tests/perf_targets_sqlite.toml`.
- Performance runs are release-profile (`cargo test --release`) and are calibrated
  against the local machine baseline.
- SQLite track runs in phase-1 report-only mode (`enforcement_mode = "report_only"`).
- `scripts/system_tests/perf_calibrate.py` recalibrates thresholds from repeated runs.
- `scripts/system_tests/perf_analyze.py` aggregates artifacts to rank bottlenecks.
- `scripts/system_tests/test_runner.py` enforces `min_executed_tests` (default `1`)
  to fail selector mismatches that execute zero tests.

## Hygiene Rules

- No fail-open checks.
- No sleeps for correctness (use readiness probes).
- Use canonical types from production crates.
- Always update the test registry when adding or removing tests.
"""
    return guide


def main() -> None:
    """CLI entry point for documentation generation."""
    parser = argparse.ArgumentParser(description="Generate Decision Gate test coverage docs")
    parser.add_argument("action", choices=["generate"], help="Generate documentation")
    parser.parse_args()

    workspace_root = Path(__file__).resolve().parents[2]
    registry_path = workspace_root / "system-tests" / "test_registry.toml"
    gaps_path = workspace_root / "system-tests" / "test_gaps.toml"
    out_dir = workspace_root / "Docs" / "testing"
    out_dir.mkdir(parents=True, exist_ok=True)

    registry = load_toml(registry_path)
    gaps = load_toml(gaps_path)

    coverage = generate_coverage_report(registry, gaps)
    infra = generate_infrastructure_guide(registry)

    (out_dir / "decision_gate_test_coverage.md").write_text(coverage, encoding="utf-8")
    (out_dir / "test_infrastructure_guide.md").write_text(infra, encoding="utf-8")

    print(f"Wrote: {out_dir / 'decision_gate_test_coverage.md'}")
    print(f"Wrote: {out_dir / 'test_infrastructure_guide.md'}")


if __name__ == "__main__":
    main()
