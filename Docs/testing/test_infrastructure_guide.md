# Decision Gate Test Infrastructure Guide (Auto-Generated)

**Generated:** 2026-02-14 17:57:35 UTC

> This document is auto-generated by `scripts/system_tests/coverage_report.py`.

## Overview

System-tests are registry-driven. Every test is declared in
`system-tests/test_registry.toml` and all known gaps live in
`system-tests/test_gaps.toml`.

## Running Tests

```bash
python scripts/system_tests/test_runner.py --priority P0
python scripts/system_tests/test_runner.py --category runpack
python scripts/system_tests/test_runner.py --category performance
```

## Environment Variables

- `DECISION_GATE_SYSTEM_TEST_RUN_ROOT`: per-test artifact root (set by runner)
- `DECISION_GATE_SYSTEM_TEST_HTTP_BIND`: optional bind override
- `DECISION_GATE_SYSTEM_TEST_PROVIDER_URL`: optional external provider URL
- `DECISION_GATE_SYSTEM_TEST_TIMEOUT_SEC`: optional timeout override

## Category Index

| Category | Description | Quick |
| --- | --- | --- |
| smoke | Fast sanity checks | yes |
| functional | Feature validation and workflows | yes |
| providers | Evidence providers and federation | yes |
| mcp_transport | MCP transport validation | yes |
| runpack | Runpack export/verify integrity | yes |
| reliability | Determinism and idempotency | no |
| security | Disclosure and policy enforcement | yes |
| contract | Schema and contract conformance | yes |
| operations | Startup and configuration validation | no |
| performance | Performance throughput gates | no |
| agentic | Agentic flow harness scenarios | no |

## Artifact Contract

Each system-test emits at least:
- `summary.json`
- `summary.md`
- `tool_transcript.json`

Runpack tests also emit `runpack/` with exported artifacts.

Performance tests also emit:
- `perf_summary.json` (throughput/latency/error metrics + SLO evaluation)
- `perf_tool_metrics.json` (per-tool p95 and total-time rankings)
- `perf_target.json` (resolved threshold/workload contract for the run)

## Performance Operating Model

- Absolute SLO thresholds are defined in `system-tests/perf_targets.toml`.
- Performance runs are release-profile (`cargo test --release`) and are calibrated
  against the local machine baseline.
- `scripts/system_tests/perf_calibrate.py` recalibrates thresholds from repeated runs.
- `scripts/system_tests/perf_analyze.py` aggregates artifacts to rank bottlenecks.
- `scripts/system_tests/test_runner.py` enforces `min_executed_tests` (default `1`)
  to fail selector mismatches that execute zero tests.

## Hygiene Rules

- No fail-open checks.
- No sleeps for correctness (use readiness probes).
- Use canonical types from production crates.
- Always update the test registry when adding or removing tests.
